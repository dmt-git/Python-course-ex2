{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5ccddc7-1525-47a0-ace6-8c19aeafc386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "import asyncio\n",
    "from aiohttp import ClientSession, TCPConnector\n",
    "\n",
    "data_path = Path(Path.cwd(), 'data', 'hw1.db')\n",
    "url_search = 'https://hh.ru/search/vacancy'\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "url_api = 'https://api.hh.ru/vacancies'\n",
    "url_abs = 'https://api.hh.ru'\n",
    "\n",
    "params = {'area': '113'\n",
    "       ,'text': 'middle python developer'\n",
    "       ,'search_field': 'description'\n",
    "       ,'professional_role': '96'\n",
    "       ,'items_on_page': '20'\n",
    "       ,'page': '0'\n",
    "      }\n",
    "table_name1, table_name2, table_name3 = 'vacancies1', 'vacancies2', 'vacancies3'\n",
    "#Реализация заполнения двух таблиц vacancies1, vacancies2 одинаковой структуры\n",
    "#вакансиями с hh.ru\n",
    "#Таблицы должны быть созданы заранее в базе данных SQLite: hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d69a866-5f38-4b6e-8695-1e7ad510e9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Данные из html-странички, запись в таблицу vacancies1\n",
    "def first_method():\n",
    "    links = []\n",
    "    page = 0\n",
    "    data_list = []\n",
    "    #Составляем список 100 ссылок на вакансии\n",
    "    while (len(data_list) < 100) and (page < 10):\n",
    "        params['page'] = str(page)\n",
    "        result = requests.get(url_search, headers = user_agent, params = params)\n",
    "        if result.status_code != 200:\n",
    "            page += 1\n",
    "            continue\n",
    "        soup = BeautifulSoup(result.content, 'lxml')\n",
    "    \n",
    "        links = [l['href'].partition('?')[0] for l in soup.find_all('a', attrs={'data-qa':\"serp-item__title\"})]\n",
    "        page += 1\n",
    "    \n",
    "        for url_vacancy in links:\n",
    "            result = requests.get(url_vacancy, headers=user_agent)\n",
    "            if result.status_code != 200:\n",
    "                continue\n",
    "            soup = BeautifulSoup(result.content, 'lxml')\n",
    "            dic_str = {}\n",
    "            #Название компании       \n",
    "            dic_str['company_name'] = soup.find('span', attrs={'data-qa':\"bloko-header-2\"}).get_text()\n",
    "            #Название позиции\n",
    "            dic_str['position'] = soup.find('h1', attrs={'data-qa':\"vacancy-title\"}).get_text()\n",
    "            #Описание вакансии\n",
    "            dic_str['job_description'] = soup.find('div', attrs={'data-qa':\"vacancy-description\"}).get_text()\n",
    "            #Ключевые навыки\n",
    "            dic_str['key_skills'] = '; '.join([s.get_text() for s \n",
    "                                               in soup.find_all('span', attrs={'data-qa':\"bloko-tag__text\"})\n",
    "                                              ])\n",
    "            if 'Python' not in dic_str['key_skills']: #Не берём, если отсутствует требование по Python\n",
    "                continue\n",
    "            data_list.append(dic_str)\n",
    "            time.sleep(0.5) #Если делать без паузы, то через много страниц может выдать ошибку страницы\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    try:\n",
    "        connection = sqlite3.connect(data_path)\n",
    "    except:\n",
    "        print(f\"Error: Проблема с файлом '{data_path}'\")\n",
    "    try:\n",
    "        df.to_sql(table_name1, connection, if_exists='replace', index=False)\n",
    "        print('1st method: ok')\n",
    "    except:\n",
    "        print(f\"Error: Проблема с записью таблицы {table_name1}\")\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c51f371d-96e1-4abd-b003-433fcfeeed15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Данные по api\n",
    "def second_method():\n",
    "    links = []\n",
    "    page = 0\n",
    "    data_list = []\n",
    "    #Составляем список 100 ссылок на вакансии\n",
    "    while (len(data_list) < 100) and (page < 10):\n",
    "        params['page'] = str(page)\n",
    "        result = requests.get(url_api, params = params, headers = user_agent)\n",
    "        if result.status_code != 200:\n",
    "            page += 1\n",
    "            continue\n",
    "        result_json = result.json()\n",
    "        links = [l['url'].split('?')[0] for l in result_json['items']]\n",
    "        page += 1\n",
    "    \n",
    "        for url_vacancy in links:\n",
    "            result2 = requests.get(url_vacancy, headers = user_agent)\n",
    "            if result.status_code != 200:\n",
    "                continue\n",
    "    \n",
    "            result2_json = result2.json()\n",
    "            dic_str = {}\n",
    "            #Название компании       \n",
    "            dic_str['company_name'] = result2_json['employer']['name']\n",
    "            #Название позиции\n",
    "            dic_str['position'] = result2_json['name']\n",
    "            #Описание вакансии\n",
    "            dic_str['job_description'] = BeautifulSoup(result2_json['description'], 'lxml').get_text()\n",
    "            #Ключевые навыки\n",
    "            dic_str['key_skills'] = '; '.join([s['name'] for s in result2_json[\"key_skills\"]])\n",
    "            if 'Python' not in dic_str['key_skills']:  #Не берём, если отсутствует требование по Python             \n",
    "                continue\n",
    "            data_list.append(dic_str)\n",
    "            time.sleep(0.5) #Если делать без паузы, то через много страниц может выдать ошибку страницы\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    try:\n",
    "        connection = sqlite3.connect(data_path)\n",
    "    except:\n",
    "        print(f\"Error: Проблема с файлом '{data_path}'\")\n",
    "    try:\n",
    "        df.to_sql(table_name2, connection, if_exists='replace', index=False)\n",
    "        print('2nd method: ok')\n",
    "    except:\n",
    "        print(f\"Error: Проблема с записью таблицы {table_name2}\")\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5edc3a8-0baf-484d-8f42-7b9a3f8cc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_list_vacancies(page, session):\n",
    "    url = f'/vacancies/'\n",
    "    params = {'area': '113'\n",
    "       ,'text': 'middle python developer'\n",
    "       ,'search_field': 'description'\n",
    "       ,'professional_role': '96'\n",
    "       ,'items_on_page': '20'\n",
    "       ,'page': str(page)\n",
    "      }\n",
    "    async with session.get(url=url, params=params) as response:\n",
    "        result_json = await response.json()\n",
    "        num_pages = int(result_json['pages'])\n",
    "        if page < num_pages:\n",
    "            return [l['url'].split('vacancies/')[1].split('?')[0] for l in result_json['items']]\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cc9bf5b-b213-453b-9b67-60c4ae048378",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_vacancy(link_id, session):\n",
    "    url = f'/vacancies/' + link_id\n",
    "    keys=['employer', 'name', 'description', 'key_skills']\n",
    "    async with session.get(url=url) as response:\n",
    "        result_json = await response.json()\n",
    "        if len(result_json.keys())>40: #Проверяем, подгрузилась ли полноценная страничка вакансии\n",
    "            dic_str = {}\n",
    "            #Название компании       \n",
    "            dic_str['company_name'] = result_json['employer']['name']\n",
    "            #Название позиции\n",
    "            dic_str['position'] = result_json['name']\n",
    "            #Описание вакансии\n",
    "            dic_str['job_description'] = BeautifulSoup(result_json['description'], 'lxml').get_text()\n",
    "            #Ключевые навыки\n",
    "            dic_str['key_skills'] = '; '.join([s['name'] for s in result_json['key_skills']])\n",
    "            if 'Python' not in dic_str['key_skills']:              \n",
    "                return {} #Не берём, если отсутствует требование по Python \n",
    "            else:\n",
    "                return dic_str\n",
    "        else:\n",
    "            return {} # Если страничка не подгрузилась или капча\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d93373ed-d9ff-47e1-a88b-6fbfe61afae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def third_method():\n",
    "    #По api асинхронными методами\n",
    "    links = []\n",
    "    data_list = []\n",
    "    connector = TCPConnector(limit=4) #Больше чем 4 почему-то вообще данные не выдаёт\n",
    "    async with ClientSession(url_abs, connector=connector) as session:\n",
    "        tasks = []\n",
    "        # Собираем ссылки на вакансии\n",
    "        for page in range(10):\n",
    "            tasks.append(asyncio.create_task(get_list_vacancies(page, session)))\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for r in results:\n",
    "            links += r\n",
    "        #По списку ссылок собираем информацию по вакансиям\n",
    "        tasks = []\n",
    "        for link_id in links:\n",
    "            tasks.append(asyncio.create_task(get_vacancy(link_id, session)))\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        data_list = [d for d in results if len(d) > 0] #Убираем пустые вакансии       \n",
    "        #Записываем в табличку собранную информацию\n",
    "        df = pd.DataFrame(data_list)\n",
    "        try:\n",
    "            connection = sqlite3.connect(data_path)\n",
    "        except:\n",
    "            print(f\"Error: Проблема с файлом '{data_path}'\")\n",
    "        try:\n",
    "            df.to_sql(table_name3, connection, if_exists='replace', index=False)\n",
    "            print('3d method: ok')\n",
    "        except:\n",
    "            print(f\"Error: Проблема с записью таблицы {table_name3}\")\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4da859-091a-4c3e-a255-962e621c31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_method()\n",
    "second_method()\n",
    "await third_method() #через 120 запросов выдаётся капча, после отбрасывания части вакансий остаётся максимум 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df844951-294f-4527-981c-9cafcf96b320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
